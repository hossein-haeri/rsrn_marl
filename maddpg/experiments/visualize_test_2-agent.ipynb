{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import seaborn as sns\n",
    "\n",
    "BLUE = [0, 0.4470, 0.7410]\n",
    "RED = [0.8500, 0.3250, 0.0980]\n",
    "YELLOW = [0.929, 0.6940, 0.1250]\n",
    "agent_colors = [BLUE, RED, YELLOW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "To load the data, directory 'saved_policy' must be in the same directory as this notebook.\n",
    "2-agent results are evaluated for 3 number of train runs and 3-agent results are evaluated for 10 number of train runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "Run 10\n",
      "Calculating distances and rewards for agent 1...\n",
      "Calculating distances and rewards for agent 2...\n",
      "Calculating distances and rewards for agent 3...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "num_agents = 3\n",
    "num_train_runs = 10\n",
    "\n",
    "# the rewards are not saved in the trajectory to make the logs lighter. We need to calculate them here\n",
    "# this is the reward function used in the training if you change the reward function in the training you need to change it here as well\n",
    "def reward_function(d):\n",
    "    return np.maximum(np.exp(-(d ** 2) / .1), 0.01)\n",
    "\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    return np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
    "\n",
    "\n",
    "if num_agents == 3:\n",
    "    rsrn_types = ['WPM']\n",
    "    agent_limitations = ['slow']\n",
    "    networks = ['self-interested', 'fully-connected', 'authoritarian', 'collapsed authoritarian', 'tribal', 'collapsed tribal']\n",
    "if num_agents == 2:\n",
    "    rsrn_types = ['WSM', 'Minmax', 'WPM']\n",
    "    agent_limitations = ['normal','slow', 'stuck']\n",
    "    networks = ['fully-connected']\n",
    "\n",
    "# Data loading and DataFrame creation\n",
    "rows = []\n",
    "for i in range(num_train_runs):\n",
    "    print(f'Run {i+1}')\n",
    "    for rsrn_type in rsrn_types:\n",
    "        for network in networks:\n",
    "            for agent_limitation in agent_limitations:\n",
    "                path = f'./saved_policy/{num_agents}-agent_{rsrn_type}_{network}_{agent_limitation}_{i+1}/'\n",
    "                with gzip.open(path + 'test_trajectory.pkl.gz', 'rb') as f:\n",
    "                    run = pickle.load(f)\n",
    "                    info = [rsrn_type, network, agent_limitation, i + 1]\n",
    "                    for trajectory in run:\n",
    "                        for row in trajectory:\n",
    "                            rows.append(row + info)\n",
    "\n",
    "# Now convert this into a pandas DataFrame\n",
    "if num_agents == 2:\n",
    "    df = pd.DataFrame(rows, columns=[   'episode', \n",
    "                                        'timestep', \n",
    "                                        'agent_1_x', 'agent_1_y', \n",
    "                                        'agent_2_x', 'agent_2_y', \n",
    "                                        'landmark_1_x', 'landmark_1_y', \n",
    "                                        'landmark_2_x', 'landmark_2_y', \n",
    "                                        'agent_1_reward', \n",
    "                                        'agent_2_reward',\n",
    "                                        'rsrn_type',\n",
    "                                        'network',\n",
    "                                        'agent_limitation',\n",
    "                                        'run_number'])\n",
    "\n",
    "# Now convert this into a pandas DataFrame\n",
    "if num_agents == 3:\n",
    "    df = pd.DataFrame(rows, columns=[   'episode', \n",
    "                                        'timestep', \n",
    "                                        'agent_1_x', 'agent_1_y', \n",
    "                                        'agent_2_x', 'agent_2_y', \n",
    "                                        'agent_3_x', 'agent_3_y', \n",
    "                                        'landmark_1_x', 'landmark_1_y', \n",
    "                                        'landmark_2_x', 'landmark_2_y',\n",
    "                                        'landmark_3_x', 'landmark_3_y',\n",
    "                                        'agent_1_reward', \n",
    "                                        'agent_2_reward',\n",
    "                                        'agent_3_reward',\n",
    "                                        'rsrn_type',\n",
    "                                        'network',\n",
    "                                        'agent_limitation',\n",
    "                                        'run_number'])\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "# Replace 'Minmax' with 'MiniMax' in 'rsrn_type' column\n",
    "df['rsrn_type'] = df['rsrn_type'].replace('Minmax', 'MiniMax')\n",
    "del rows\n",
    "\n",
    "# Calculating distances and rewards\n",
    "for agent_index in range(1, num_agents + 1):\n",
    "    print(f'Calculating distances and rewards for agent {agent_index}...')\n",
    "    agent_x = df[f'agent_{agent_index}_x'].values\n",
    "    agent_y = df[f'agent_{agent_index}_y'].values\n",
    "\n",
    "    distances = np.array([euclidean_distance(agent_x, agent_y, df[f'landmark_{j}_x'].values, df[f'landmark_{j}_y'].values) for j in range(1, num_agents + 1)])\n",
    "    closest_distances = distances.min(axis=0)\n",
    "\n",
    "    df[f'agent_{agent_index}_closest_landmark_distance'] = closest_distances\n",
    "    df[f'agent_{agent_index}_indv_reward'] = reward_function(closest_distances)\n",
    "\n",
    "# Cumulative rewards\n",
    "grouping_columns = ['rsrn_type', 'network', 'agent_limitation', 'run_number', 'episode']\n",
    "for agent_index in range(1, num_agents + 1):\n",
    "    df[f'agent_{agent_index}_cum_reward'] = df.groupby(grouping_columns)[f'agent_{agent_index}_reward'].cumsum()\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Agent Behavioral Analysis (Different Networks - Slow - WPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "\n",
    "\n",
    "variable_of_interest = 'indv_reward' #'closest_landmark_distance', indv_reward\n",
    "\n",
    "\n",
    "\n",
    "BLUE = [0, 0.4470, 0.7410]\n",
    "RED = [0.8500, 0.3250, 0.0980]\n",
    "YELLOW = [0.929, 0.6940, 0.1250]\n",
    "p = [BLUE,RED, YELLOW]\n",
    "sns.set_palette(p)\n",
    "plt.close('all')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "def plot_episode(variable_of_interest, num_agents, rsrn_types, networks, agent_limitation='slow'):\n",
    "    # sns.set_style(\"whitegrid\")\n",
    "    # sns.set_context(\"paper\")\n",
    "    # sns.set_palette(\"colorblind\")\n",
    "    sns.set(font_scale=1)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(6, 15))\n",
    "    axs_ = axs.flat\n",
    "    for j, network in enumerate(networks):\n",
    "        if j == 0: letter = '(a) Self-interested'\n",
    "        if j == 1: letter = '(b) Communitarian'\n",
    "        if j == 2: letter = '(c) Authoritarian'\n",
    "        if j == 3: letter = '(d) Collapsed Authoritarian'\n",
    "        if j == 4: letter = '(e) Tribal'\n",
    "        if j == 5: letter = '(f) Collapsed Tribal'\n",
    "\n",
    "        for i, rsrn_type in enumerate(rsrn_types):\n",
    "            # get ax as the jth element \n",
    "            ax = axs_[j]\n",
    "            df_ep = df.loc[(df['rsrn_type'] == rsrn_type) & (df['network'] == network) & (df['agent_limitation'] == agent_limitation)]\n",
    "            # df_ep = df.loc[df['network'] == network]\n",
    "            # print(df_ep.head())\n",
    "            for agent_index in range(1, num_agents+1):\n",
    "                # sns.lineplot(data=df_ep, x='timestep', y=f'agent_{agent_index}_{variable_of_interest}', ax=axs[i,j], label=f'Agent {agent_index}',errorbar='sd')\n",
    "                # if last agent append to its legend agent_limitation\n",
    "                if agent_index == num_agents: label = f'Agent {agent_index} ({agent_limitation})'\n",
    "                else: label = f'Agent {agent_index} (normal)'\n",
    "\n",
    "                sns.lineplot(data=df_ep, x='timestep', \n",
    "                                y=f'agent_{agent_index}_{variable_of_interest}', \n",
    "                                ax=ax,\n",
    "                                label=label,\n",
    "                                errorbar=('pi',50), \n",
    "                                color=p[agent_index-1],\n",
    "                                estimator=np.median)\n",
    "                sns.lineplot(data=df_ep, x='timestep',\n",
    "                             y=f'agent_{agent_index}_{variable_of_interest}',\n",
    "                              ax=ax,\n",
    "                              legend=False, errorbar=None, \n",
    "                              estimator=np.mean, \n",
    "                              linestyle='--', \n",
    "                              color=agent_colors[agent_index-1], \n",
    "                              alpha=0.3)\n",
    "                \n",
    "            ax.set_title(letter, fontsize=14, loc='center', pad=5)\n",
    "            \n",
    "            ax.set_xlim([0, 69])\n",
    "            ax.set_xlabel('Timestep', color='gray')\n",
    "            # ax.set_ylabel('Dist. to Closest Landmark', color='gray')\n",
    "            ax.set_ylabel('Individual Reward', color='gray')\n",
    "\n",
    "            if j == 0:\n",
    "                ax.legend(loc='upper left', bbox_to_anchor=(.8, 1.6), title=f'WPM Scalarization', title_fontsize=12)\n",
    "            else:\n",
    "                ax.legend().remove()\n",
    "\n",
    "            ax.set_ylim([0, 1.3])\n",
    "            # set color of the ticks and tick labels to gray\n",
    "            ax.tick_params(axis='x', colors='gray')\n",
    "            ax.tick_params(axis='y', colors='gray')\n",
    "\n",
    "            plt.xticks(color='gray')\n",
    "            plt.yticks(color='gray')\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.375, wspace=0.37, bottom=0.1)\n",
    "    plt.show()\n",
    "plot_episode(   variable_of_interest,\n",
    "                num_agents=3,\n",
    "                rsrn_types=['WPM'],\n",
    "                networks=[  'self-interested',\n",
    "                            'fully-connected',\n",
    "                            'authoritarian',\n",
    "                            'collapsed authoritarian',\n",
    "                            'tribal',\n",
    "                            'collapsed tribal'],\n",
    "                agent_limitation='slow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Agent Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "num_agents = 3\n",
    "num_train_runs = 10\n",
    "rsrn_types = ['WPM']\n",
    "agent_limitations = ['slow']\n",
    "networks=[  \n",
    "            'self-interested',\n",
    "            'fully-connected',\n",
    "            'authoritarian',\n",
    "            'collapsed authoritarian',\n",
    "            'tribal',\n",
    "            'collapsed tribal'\n",
    "            ]\n",
    "num_episodes = 500000\n",
    "\n",
    "BLUE = [0, 0.4470, 0.7410]\n",
    "RED = [0.8500, 0.3250, 0.0980]\n",
    "YELLOW = [0.929, 0.6940, 0.1250]\n",
    "p = [BLUE,RED, YELLOW]\n",
    "sns.set_palette(p)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "plt.close('all')\n",
    "fig, axs = plt.subplots(len(networks), 3, figsize=(7, 12))\n",
    "\n",
    "for n, network in enumerate(networks):\n",
    "    cumulative_shared_reward = np.zeros((num_train_runs, num_episodes, num_agents))\n",
    "    final_distance_to_landmark = np.zeros((num_train_runs, num_episodes, num_agents))\n",
    "    cumulative_individual_reward = np.zeros((num_train_runs, num_episodes, num_agents))\n",
    "\n",
    "    for i in range(num_train_runs):\n",
    "        for rsrn_type in rsrn_types:\n",
    "                for agent_limitation in agent_limitations:\n",
    "                    path = ('./saved_policy/' + str(num_agents) + '-agent_' + rsrn_type + '_' + network + '_' + agent_limitation + '_' + str(i+1) + '/')\n",
    "                    with gzip.open(path+'train_log.pkl.gz', 'rb') as f:\n",
    "                        run = pickle.load(f)\n",
    "                        cumulative_individual_reward[i,:,:] = run['cum_individual_rewards']\n",
    "                        final_distance_to_landmark[i,:,:] = run['final_dis2landmark']\n",
    "                        cumulative_shared_reward[i,:,:] = run['cum_shared_rewards']\n",
    "\n",
    "\n",
    "    cumulative_individual_reward = cumulative_individual_reward.mean(axis=0)\n",
    "    final_distance_to_landmark = final_distance_to_landmark.mean(axis=0)\n",
    "    cumulative_shared_reward = cumulative_shared_reward.mean(axis=0)\n",
    "\n",
    "    cumulative_individual_reward_std = np.std(cumulative_individual_reward, axis=0)\n",
    "    final_distance_to_landmark_std = np.std(final_distance_to_landmark, axis=0)\n",
    "    cumulative_shared_reward_std = np.std(cumulative_shared_reward, axis=0)\n",
    "\n",
    "    chunk_length = 1000\n",
    "\n",
    "    cumulative_individual_reward_chunks = np.zeros((500, num_agents))\n",
    "    final_distance_to_landmark_chunks = np.zeros((500, num_agents))\n",
    "    cumulative_shared_reward_chunks = np.zeros((500, num_agents))\n",
    "\n",
    "    cumulative_individual_reward_chunks_std = np.zeros((500, num_agents))\n",
    "    final_distance_to_landmark_chunks_std = np.zeros((500, num_agents))\n",
    "    cumulative_shared_reward_chunks_std = np.zeros((500, num_agents))\n",
    "\n",
    "    for i in range(500):\n",
    "        cumulative_individual_reward_chunks[i,:] = np.mean(cumulative_individual_reward[i*chunk_length:(i+1)*chunk_length,:], axis=0)\n",
    "        cumulative_shared_reward_chunks[i,:] = np.mean(cumulative_shared_reward[i*chunk_length:(i+1)*chunk_length,:], axis=0)\n",
    "        final_distance_to_landmark_chunks[i,:] = np.mean(final_distance_to_landmark[i*chunk_length:(i+1)*chunk_length,:], axis=0)\n",
    "        \n",
    "        cumulative_individual_reward_chunks_std[i,:] = np.std(cumulative_individual_reward[i*chunk_length:(i+1)*chunk_length,:], axis=0)\n",
    "        cumulative_shared_reward_chunks_std[i,:] = np.std(cumulative_shared_reward[i*chunk_length:(i+1)*chunk_length,:], axis=0)\n",
    "        final_distance_to_landmark_chunks_std[i,:] = np.std(final_distance_to_landmark[i*chunk_length:(i+1)*chunk_length,:], axis=0)\n",
    "        \n",
    "    for i in range(num_agents):\n",
    "        # axs[0].scatter(range(num_episodes), cumulative_individual_reward[:,i], s=1, alpha=0.005, color=p[i])\n",
    "        # axs[1].scatter(range(num_episodes), cumulative_shared_reward[:,i], s=1, alpha=0.005, color=p[i])\n",
    "        # axs[0].scatter(range(num_episodes), cumulative_individual_reward[:,i], lw=1, alpha=0.005, color=p[i])\n",
    "\n",
    "        axs[n,0].plot(range(500), cumulative_individual_reward_chunks[:,i], lw=1, alpha=0.8, color=p[i])\n",
    "        axs[n,1].plot(range(500), cumulative_shared_reward_chunks[:,i], lw=1, alpha=0.8, color=p[i])\n",
    "        axs[n,2].plot(range(500), final_distance_to_landmark_chunks[:,i], lw=1, alpha=0.8, color=p[i])\n",
    "\n",
    "        axs[n,0].fill_between(range(500),   cumulative_individual_reward_chunks[:,i] - cumulative_individual_reward_chunks_std[:,i],\n",
    "                                            cumulative_individual_reward_chunks[:,i] + cumulative_individual_reward_chunks_std[:,i],\n",
    "                                            color=p[i], alpha=0.2, linewidth=0.0)\n",
    "        axs[n,1].fill_between(range(500),   cumulative_shared_reward_chunks[:,i] - cumulative_shared_reward_chunks_std[:,i],\n",
    "                                            cumulative_shared_reward_chunks[:,i] + cumulative_shared_reward_chunks_std[:,i],\n",
    "                                            color=p[i], alpha=0.2, linewidth=0.0)                                      \n",
    "        axs[n,2].fill_between(range(500),   final_distance_to_landmark_chunks[:,i] - final_distance_to_landmark_chunks_std[:,i],\n",
    "                                            final_distance_to_landmark_chunks[:,i] + final_distance_to_landmark_chunks_std[:,i], \n",
    "                                            color=p[i], alpha=0.2, linewidth=0.0)\n",
    "    label_font_size = 8\n",
    "    axs[n,0].set_ylabel('Individual Reward', fontsize=label_font_size)\n",
    "    axs[n,0].set_xlabel('Episode (x1000)', fontsize=label_font_size)\n",
    "    axs[n,0].axhline(y=70, color='r', linestyle='--')\n",
    "    axs[n,0].set_xlim([0, 500])\n",
    "\n",
    "    axs[n,1].set_ylabel('Relational Reward', fontsize=label_font_size)\n",
    "    axs[n,1].set_xlabel('Episode (x1000)', fontsize=label_font_size)\n",
    "    axs[n,1].axhline(y=70, color='r', linestyle='--')\n",
    "    axs[n,1].set_xlim([0, 500])\n",
    "\n",
    "    axs[n,2].set_ylabel('Dist. to Landmark ', fontsize=label_font_size)\n",
    "    axs[n,2].set_xlabel('Episode (x1000)', fontsize=label_font_size)\n",
    "    axs[n,2].set_ylim([0, 1.3])\n",
    "    axs[n,2].set_xlim([0, 500])\n",
    "\n",
    "fs = 11\n",
    "axs[0,1].set_title('(a) Survivalist (or Self-interested)', fontsize=fs,loc='center')\n",
    "axs[1,1].set_title('(b) Communitarian (or Fully-connected)', fontsize=fs,loc='center')\n",
    "axs[2,1].set_title('(c) Authoritarian', fontsize=fs,loc='center')\n",
    "axs[3,1].set_title('(d) Collapsed Authoritarian', fontsize=fs,loc='center')\n",
    "axs[4,1].set_title('(e) Tribal', fontsize=fs,loc='center')\n",
    "axs[5,1].set_title('(f) Collapsed Tribal', fontsize=fs,loc='center')\n",
    "\n",
    "# set label font size to 8\n",
    "for ax in axs.flat:\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=8)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Agent Behavioral Analysis (Different Scalarization - Fully-connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variable_of_interest = 'closest_landmark_distance' #'closest_landmark_distance', indv_reward\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.close('all')\n",
    "def plot_episode(variable_of_interestm, num_agents, rsrn_types, agent_limitations):\n",
    "    sns.set(font_scale=1)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axs = plt.subplots(len(agent_limitations), len(rsrn_types), figsize=(9, 9))\n",
    "    \n",
    "    for j, agent_limitation in enumerate(agent_limitations):\n",
    "        for i, rsrn_type in enumerate(rsrn_types):\n",
    "            df_ep = df.loc[(df['rsrn_type'] == rsrn_type) & (df['network'] == network) & (df['agent_limitation'] == agent_limitation)]\n",
    "            for agent_index in range(1, num_agents+1):\n",
    "\n",
    "\n",
    "                if agent_index == num_agents: label = f'Agent {agent_index} ({agent_limitation})'\n",
    "                else: label = f'Agent {agent_index} (normal)'\n",
    "\n",
    "                sns.lineplot(data=df_ep, x='timestep', \n",
    "                                y=f'agent_{agent_index}_{variable_of_interest}', \n",
    "                                ax=axs[i,j],\n",
    "                                label=label,\n",
    "                                errorbar=('pi',50), \n",
    "                                color=p[agent_index-1],\n",
    "                                estimator=np.median)\n",
    "                sns.lineplot(data=df_ep, x='timestep',\n",
    "                                y=f'agent_{agent_index}_{variable_of_interest}',\n",
    "                                ax=axs[i,j],\n",
    "                                legend=False, errorbar=None, \n",
    "                                estimator=np.mean, \n",
    "                                linestyle='--', \n",
    "                                color=agent_colors[agent_index-1], \n",
    "                                alpha=0.3)\n",
    "\n",
    "\n",
    "            axs[i,j].set_title(f'{rsrn_type}', fontsize=14, loc='center', pad=-20)\n",
    "            \n",
    "            axs[i,j].set_xlabel('Timestep', color='gray')\n",
    "            if variable_of_interest == 'closest_landmark_distance': axs[i,j].set_ylabel('Dist. to Closest Landmark', color='gray')\n",
    "            if variable_of_interest == 'indv_reward': axs[i,j].set_ylabel('Individual Reward', color='gray')\n",
    "\n",
    "            axs[i,j].tick_params(axis='x', colors='gray')\n",
    "            axs[i,j].tick_params(axis='y', colors='gray')\n",
    "\n",
    "            if i == 0:\n",
    "                axs[i,j].legend(loc='upper left', bbox_to_anchor=(0.1, 1.6), title=f'Case {j+1}', title_fontsize=12)\n",
    "            else:\n",
    "                axs[i,j].legend().remove()\n",
    "            axs[i,j].set_xlim([0, 69])\n",
    "            \n",
    "    # make all ylims the same as the first plot times 1.2\n",
    "    ylim = axs[0,0].get_ylim()[1]*1.3\n",
    "\n",
    "    for i, agent_limitation in enumerate(agent_limitations):\n",
    "        for j, rsrn_type in enumerate(rsrn_types):\n",
    "            axs[i,j].set_ylim([0, ylim])\n",
    "    # make extra room on the right side for legend\n",
    "    fig.subplots_adjust(top=0.86,\n",
    "                        bottom=0.064,\n",
    "                        left=0.081,\n",
    "                        right=0.98,\n",
    "                        hspace=0.285,\n",
    "                        wspace=0.421)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_episode(variable_of_interest, num_agents=2, rsrn_types=['WSM', 'MiniMax', 'WPM'], agent_limitations=['normal', 'slow', 'stuck'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Agent Heatmap of First/Last Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n",
      "(50, 50)\n",
      "(50, 50)\n",
      "(50, 50)\n",
      "(50, 50)\n",
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "# Function to create a normalized heatmap for a given agent's locations at a specific timestep\n",
    "def plot_normalized_heatmap_at_timestep(df, ax, agent_number, timestep_of_interest, setting_filters):\n",
    "    # Construct column names based on the agent number\n",
    "    agent_x_col = f'agent_{agent_number}_x'\n",
    "    agent_y_col = f'agent_{agent_number}_y'\n",
    "    bins=(50, 50)\n",
    "    # Apply setting filters to the DataFrame\n",
    "    df_filtered = df\n",
    "    for column, value in setting_filters.items():\n",
    "        df_filtered = df_filtered[df_filtered[column] == value]\n",
    "    \n",
    "    # Filter for the specific timestep of interest\n",
    "    df_filtered_at_timestep = df_filtered[df_filtered['timestep'] == timestep_of_interest]\n",
    "\n",
    "    # Calculate the total number of episodes\n",
    "    total_episodes = df_filtered_at_timestep['episode'].nunique()*10 # 10 is for number of runs\n",
    "\n",
    "    # Number of bins for x and y\n",
    "    x_num_bins = bins[0]\n",
    "    y_num_bins = bins[1]\n",
    "\n",
    "    # Create an array of bin edges from -1.2 to 1.2\n",
    "    x_bin_edges = np.linspace(-1.2, 1.2, x_num_bins + 1)\n",
    "    y_bin_edges = np.linspace(-1.2, 1.2, y_num_bins + 1)\n",
    "\n",
    "    # Bin the x and y positions\n",
    "    x_bins = pd.cut(df_filtered_at_timestep[agent_x_col], bins=x_bin_edges, labels=range(bins[0]))\n",
    "    y_bins = pd.cut(df_filtered_at_timestep[agent_y_col], bins=y_bin_edges, labels=range(bins[1]))\n",
    "\n",
    "    # Create a DataFrame with binned x and y positions\n",
    "    binned_positions = pd.DataFrame({agent_x_col: x_bins, agent_y_col: y_bins})\n",
    "\n",
    "    # Create a full grid of all possible bin combinations\n",
    "    all_bins = pd.DataFrame(\n",
    "        [(x, y) for x in range(bins[0]) for y in range(bins[1])],\n",
    "        columns=[agent_x_col, agent_y_col]\n",
    "    )\n",
    "\n",
    "    # Group and count the binned data\n",
    "    grouped_data = binned_positions.groupby([agent_x_col, agent_y_col]).size().reset_index(name='count')\n",
    "\n",
    "    # Merge with the full grid, filling missing values with 0\n",
    "    heatmap_data = pd.merge(all_bins, grouped_data, how='left', on=[agent_x_col, agent_y_col]).fillna(0)\n",
    "\n",
    "    # Pivot the data for heatmap\n",
    "    heatmap_data_pivot = heatmap_data.pivot(agent_x_col, agent_y_col, 'count').T\n",
    "\n",
    "    # Normalize the heatmap values by the total number of episodes to get the percentage\n",
    "    heatmap_data_normalized = (heatmap_data_pivot / total_episodes) * 100\n",
    "\n",
    "    ax = sns.heatmap(heatmap_data_normalized, cmap=\"turbo\", ax=ax, vmin=0, vmax=1)  # Transpose and annotate with the percentages\n",
    "    # adjust ticks represnt actual agent position values between -1.2 and 1.2 instead of bin numbers\n",
    "    num_ticks = 7\n",
    "    x_ticks = np.linspace(0, bins[0]-1, num_ticks)\n",
    "    y_ticks = np.linspace(0, bins[1]-1, num_ticks)\n",
    "    x_ticklabels = np.linspace(-1.2, 1.2, num_ticks)\n",
    "    y_ticklabels = np.linspace(-1.2, 1.2, num_ticks)\n",
    "    # set ticks and limit decimal to .0f\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    ax.set_xticklabels(x_ticklabels.round(2))\n",
    "    ax.set_yticklabels(y_ticklabels.round(2))\n",
    "\n",
    "    # label the colorbar\n",
    "    color_bar = ax.collections[0].colorbar\n",
    "    color_bar.set_label('Percentage of Presence (%)', color='gray')\n",
    "    # set color of the colorbar ticks and tick labels to gray\n",
    "    color_bar.ax.tick_params(axis='y', colors='gray')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_title(f'Agent {agent_number} at Timestep {timestep_of_interest}')\n",
    "    # set color of the ticks and tick labels to gray\n",
    "    ax.tick_params(axis='x', colors='gray')\n",
    "    ax.tick_params(axis='y', colors='gray')\n",
    "    plt.xticks(color='gray')\n",
    "    plt.yticks(color='gray')\n",
    "    ax.axis('equal')\n",
    "    return ax\n",
    "\n",
    "plt.close('all')\n",
    "sns.set_style(\"white\")\n",
    "fig, axs = plt.subplots(3, 2, figsize=(8, 12))\n",
    "specific_setting = {\n",
    "    'rsrn_type': 'WPM',\n",
    "    'network': 'fully-connected',\n",
    "    'agent_limitation': 'slow'\n",
    "}\n",
    "for i in range(3):\n",
    "        plot_normalized_heatmap_at_timestep(df, ax=axs[i,0], agent_number=i+1, timestep_of_interest=0, setting_filters=specific_setting)\n",
    "        plot_normalized_heatmap_at_timestep(df, ax=axs[i,1], agent_number=i+1, timestep_of_interest=69, setting_filters=specific_setting)\n",
    "\n",
    "# save figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('3-agent_heatmap_v2.png', dpi=300)\n",
    "plt.savefig('3-agent_heatmap_v2.svg')\n",
    "plt.savefig('3-agent_heatmap_v2.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Agent Histogram of First/Last Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUE = [0, 0.4470, 0.7410]\n",
    "RED = [0.8500, 0.3250, 0.0980]\n",
    "YELLOW = [0.929, 0.6940, 0.1250]\n",
    "\n",
    "p = [RED, BLUE, YELLOW]\n",
    "sns.set_palette(p)\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "agent_limitations = ['normal', 'slow', 'stuck']\n",
    "rsrn_types = ['WSM', 'MiniMax', 'WPM']\n",
    "\n",
    "# for j, agent_limitation in [(0, 'normal')]:\n",
    "for j, agent_limitation in enumerate(agent_limitations):\n",
    "        fig, axs = plt.subplots(3,2, figsize=(8,10))\n",
    "        # fig.suptitle('Agent 2 limitation: {}'.format(agent_limitation), fontsize=16)\n",
    "        # fig, axs = plt.subplots(num_agents, 2, figsize=(8, 5))\n",
    "        for p, rsrn_type in enumerate(rsrn_types):\n",
    "                \n",
    "                df_ = df[(df['rsrn_type'] == rsrn_type) &\n",
    "                                (df['agent_limitation'] == agent_limitation)&\n",
    "                                (df['timestep'] == 0)\n",
    "                                ]\n",
    "                \n",
    "                # plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\n",
    "\n",
    "                melted_df = data=df_.melt(value_vars=['agent_2_closest_landmark_distance', 'agent_1_closest_landmark_distance'], var_name='agent_name', value_name='value')\n",
    "                # sort melted_df by agent_name\n",
    "                melted_df = melted_df.sort_values(by=['agent_name'])\n",
    "                sns.histplot(ax=axs[p,0],data=melted_df, x='value', hue='agent_name', element='step', common_norm=False, bins=25, stat='percent',legend=False, binrange=(0, 1.0), hue_order=['agent_2_closest_landmark_distance', 'agent_1_closest_landmark_distance'])\n",
    "                \n",
    "                axs[p,0].set_xlim([0, 1.0])\n",
    "                axs[p,0].set_ylim([0, 100.1])\n",
    "                # axs[p,0].set_ylabel('number of episodes')\n",
    "                # axs[p,0].legend(['agent 1', 'agent 2'])\n",
    "\n",
    "                df_ = df[(df['rsrn_type'] == rsrn_type) &\n",
    "                        (df['agent_limitation'] == agent_limitation)&\n",
    "                        (df['timestep'] == 69)\n",
    "                        ]\n",
    "                # plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\n",
    "                melted_df = data=df_.melt(value_vars=['agent_2_closest_landmark_distance', 'agent_1_closest_landmark_distance'], var_name='agent_name', value_name='value')\n",
    "                melted_df = melted_df.sort_values(by=['agent_name'])\n",
    "                sns.histplot(ax=axs[p,1], data=melted_df, x='value', hue='agent_name', element='step', common_norm=False, bins=25, stat='percent',legend=False, binrange=(0, 1.0), hue_order=['agent_2_closest_landmark_distance', 'agent_1_closest_landmark_distance'])\n",
    "                axs[p,1].set_xlim([0, 1.0])\n",
    "                axs[p,1].set_ylim([0, 100.1])\n",
    "                # make subplot titles\n",
    "                axs[p,0].set_title(rsrn_type)\n",
    "                axs[p,1].set_title(rsrn_type)\n",
    "                axs[p,1].set_xlabel('')\n",
    "                axs[p,0].set_xlabel('')\n",
    "                axs[p,1].set_ylabel('Percent', color='gray')\n",
    "                axs[p,0].set_ylabel('Percent', color='gray')\n",
    "                axs[p,0].tick_params(axis='x', colors='gray')\n",
    "                axs[p,0].tick_params(axis='y', colors='gray')\n",
    "                axs[p,1].tick_params(axis='x', colors='gray')\n",
    "                axs[p,1].tick_params(axis='y', colors='gray')\n",
    "\n",
    "        axs[2,1].set_xlabel('Closest landmark distance \\n (at the last timestep)', color='gray')\n",
    "        axs[2,0].set_xlabel('Closest landmark distance \\n (at the first timestep)', color='gray')\n",
    "\n",
    "        # place legend outside of the plot\n",
    "        axs[0,0].legend(['agent 1 (normal)', f'agent 2 ({agent_limitation})'], loc='upper left', bbox_to_anchor=(0.8, 1.5), title=f'Case {j+1}', title_fontsize=12,)\n",
    "        # incrase space at the bottom\n",
    "        fig.subplots_adjust(bottom=0.1, wspace=0.3, hspace=0.3)\n",
    "        # fig.tight_layout()\n",
    "        fig.savefig(f'hist_{agent_limitation}.svg', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Agent Histogram of First/Last Timestep (fully connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUE = [0, 0.4470, 0.7410]\n",
    "RED = [0.8500, 0.3250, 0.0980]\n",
    "YELLOW = [0.929, 0.6940, 0.1250]\n",
    "\n",
    "p = [RED, BLUE, YELLOW]\n",
    "sns.set_palette(p)\n",
    "\n",
    "\n",
    "agent_limitations = ['normal', 'slow', 'stuck']\n",
    "rsrn_types = ['WSM', 'MiniMax', 'WPM']\n",
    "\n",
    "# for j, agent_limitation in [(0, 'normal')]:\n",
    "for j, agent_limitation in enumerate(agent_limitations):\n",
    "        fig, axs = plt.subplots(3,2, figsize=(8,10))\n",
    "        # fig.suptitle('Agent 2 limitation: {}'.format(agent_limitation), fontsize=16)\n",
    "        # fig, axs = plt.subplots(num_agents, 2, figsize=(8, 5))\n",
    "        for p, rsrn_type in enumerate(rsrn_types):\n",
    "                \n",
    "                df_ = df[(df['rsrn_type'] == rsrn_type) &\n",
    "                                (df['agent_limitation'] == agent_limitation)&\n",
    "                                (df['timestep'] == 0)\n",
    "                                ]\n",
    "                \n",
    "                # plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\n",
    "\n",
    "                melted_df = data=df_.melt(value_vars=['agent_2_closest_landmark_distance', 'agent_1_closest_landmark_distance'], var_name='agent_name', value_name='value')\n",
    "                # sort melted_df by agent_name\n",
    "                melted_df = melted_df.sort_values(by=['agent_name'])\n",
    "                sns.histplot(ax=axs[p,0],data=melted_df, x='value', hue='agent_name', element='step', common_norm=False, bins=25, stat='percent',legend=False, binrange=(0, 1.0), hue_order=['agent_2_closest_landmark_distance', 'agent_1_closest_landmark_distance'])\n",
    "                \n",
    "                axs[p,0].set_xlim([0, 1.0])\n",
    "                axs[p,0].set_ylim([0, 100.1])\n",
    "                # axs[p,0].set_ylabel('number of episodes')\n",
    "                # axs[p,0].legend(['agent 1', 'agent 2'])\n",
    "\n",
    "                df_ = df[(df['rsrn_type'] == rsrn_type) &\n",
    "                        (df['agent_limitation'] == agent_limitation)&\n",
    "                        (df['timestep'] == 69)\n",
    "                        ]\n",
    "                # plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\n",
    "                melted_df = data=df_.melt(value_vars=['agent_2_closest_landmark_distance', 'agent_1_closest_landmark_distance'], var_name='agent_name', value_name='value')\n",
    "                melted_df = melted_df.sort_values(by=['agent_name'])\n",
    "                sns.histplot(ax=axs[p,1], data=melted_df, x='value', hue='agent_name', element='step', common_norm=False, bins=25, stat='percent',legend=False, binrange=(0, 1.0), hue_order=['agent_2_closest_landmark_distance', 'agent_1_closest_landmark_distance'])\n",
    "                axs[p,1].set_xlim([0, 1.0])\n",
    "                axs[p,1].set_ylim([0, 100.1])\n",
    "                # make subplot titles\n",
    "                axs[p,0].set_title(rsrn_type)\n",
    "                axs[p,1].set_title(rsrn_type)\n",
    "                axs[p,1].set_xlabel('')\n",
    "                axs[p,0].set_xlabel('')\n",
    "                # axs[p,0].legend(['agent 1 (normal)', f'agent 2 ({agent_limitation})'])\n",
    "                # axs[p,1].legend(['agent 1 (normal)', f'agent 2 ({agent_limitation})'])\n",
    "        axs[2,1].set_xlabel('Closest landmark distance \\n (at the last timestep)')\n",
    "        axs[2,0].set_xlabel('Closest landmark distance \\n (at the first timestep)')\n",
    "        # place legend outside of the plot\n",
    "        axs[0,0].legend(['agent 1 (normal)', f'agent 2 ({agent_limitation})'], loc='upper left', bbox_to_anchor=(0.8, 1.5), title=f'Case {j+1}', title_fontsize=12,)\n",
    "        # incrase space at the bottom\n",
    "        fig.subplots_adjust(bottom=0.1, wspace=0.3, hspace=0.3)\n",
    "        # fig.tight_layout()\n",
    "        fig.savefig(f'hist_{agent_limitation}.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(d):\n",
    "        return np.maximum(np.exp(-(d**2)/.1),0.01)\n",
    "\n",
    "def plot_reward_function(df, ax, num_agents=2):\n",
    "\n",
    "    # get the landmark locations for episode 1 timestep 1 from df\n",
    "    x = np.linspace(-1.2, 1.2, 1000)\n",
    "    y = np.linspace(-1.2, 1.2, 1000)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    # calculate landamrk locations as n landmakrs distributed evenly in a circle with radius 0.5\n",
    "    landmark_locations = []\n",
    "    for i in range(num_agents):\n",
    "        landmark_locations.append([np.cos(2*np.pi/num_agents*i), np.sin(2*np.pi/num_agents*i)])\n",
    "    landmark_locations = 0.5 * np.array(landmark_locations)\n",
    "\n",
    "    # calculate the distance to the closest landmark for each point in the grid\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(Y)):\n",
    "            dists = []\n",
    "            for k in range(num_agents):\n",
    "                dists.append(np.linalg.norm(np.array([X[i,j], Y[i,j]]) - np.array(landmark_locations[k])))\n",
    "            Z[i,j] = reward_function(min(dists))\n",
    "    # plot the reward function as a 2D heatmap using sns\n",
    "    \n",
    "    # ax = plt.axes()\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    if num_agents == 2:\n",
    "        case = '(a)'\n",
    "    elif num_agents == 3:\n",
    "        case = '(b)'\n",
    "    ax.set_title(f'{case} {num_agents}-Agent-{num_agents}-Landmark Environment')\n",
    "    cs = ax.contourf(X, Y, Z, 50, cmap='cividis')\n",
    "    if num_agents == 3:\n",
    "\n",
    "        cbar = fig.colorbar(cs)\n",
    "        # label the colorbar\n",
    "        cbar.set_label('Individual Reward')\n",
    "        # limit the colorbar to 0 to 1 and make sure 0 and 1 are part of the ticks\n",
    "        cbar.set_ticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "        # plot contour lines with labels on top of the heatmap\n",
    "    cs = ax.contour(X, Y, Z, [0.01, 0.1, 0.25, 0.5, 0.75, 0.95], colors='white', linewidths=0.5)\n",
    "    # add specific contour values\n",
    "    ax.clabel(cs, inline=1, fontsize=8)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    # set ticks and limit decimal to .0f\n",
    "    num_ticks = 7\n",
    "    x_ticks = np.linspace(-1.2, 1.2, num_ticks)\n",
    "    y_ticks = np.linspace(-1.2, 1.2, num_ticks)\n",
    "    # set ticks and limit decimal to .0f\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_yticks(y_ticks)\n",
    "\n",
    "plt.close('all')\n",
    "fig, axs = plt.subplots(1,2,figsize=(9.5,4))\n",
    "plot_reward_function(df, axs[0], num_agents=2)\n",
    "plot_reward_function(df, axs[1], num_agents=3)\n",
    "# tight layout\n",
    "fig.tight_layout()\n",
    "# save figure to file\n",
    "fig.savefig('reward_function.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the trajectory of any given episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "num_agents = 2\n",
    "r_l = 0.05  # Landmark radius\n",
    "r_a = 0.2   # Agent radius\n",
    "\n",
    "# Landmark locations\n",
    "landmark_locations = []\n",
    "for i in range(num_agents):\n",
    "    landmark_locations.append([np.cos(2*np.pi/num_agents*i), np.sin(2*np.pi/num_agents*i)])\n",
    "landmark_locations = 0.5 * np.array(landmark_locations)\n",
    "\n",
    "# Agent locations\n",
    "agent_locations = np.array([[0.56, 0.5], [-0.0, -0.0]])\n",
    "# for _ in range(num_agents):\n",
    "#     agent_locations.append([random.uniform(-1, 1), random.uniform(-1, 1)])\n",
    "# agent_locations = np.array(agent_locations)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Draw landmarks\n",
    "for location in landmark_locations:\n",
    "    landmark = plt.Circle(location, r_l, color='gray', label='Landmark' if location is landmark_locations[0] else \"\")\n",
    "    ax.add_artist(landmark)\n",
    "\n",
    "# Draw agents\n",
    "for i, location in enumerate(agent_locations):\n",
    "    agent = plt.Circle(location, r_a, color=agent_colors[i], label='Agent' if location is agent_locations[0] else \"\")\n",
    "    ax.add_artist(agent)\n",
    "\n",
    "# Set limits and aspect\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Legends\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
