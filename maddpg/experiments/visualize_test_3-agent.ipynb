{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import cv2\n",
    "import time\n",
    "import gzip\n",
    "\n",
    "BLUE = [0, 0.4470, 0.7410]\n",
    "RED = [0.8500, 0.3250, 0.0980]\n",
    "YELLOW = [0.929, 0.6940, 0.1250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "This code loads the data as a list 'trajectories' and also makes a data frame 'df' for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'fully-connected_10'\n",
    "path = './saved_policy/3-agent_WPM_authoritarian_slow_1'\n",
    "\n",
    "num_agents = 2\n",
    "num_landmarks = num_agents\n",
    "def load_data(run_name):\n",
    "    # import pickle file from ./test_policy/test_trajectory.pkl\n",
    "    # with open('./test_policy/test_'+run_name+'/test_trajectory.pkl', 'rb') as f:\n",
    "\n",
    "    # load path+'/test_trajectory.pkl.gz' file with gzip\n",
    "    with gzip.open(path+'/test_trajectory.pkl.gz', 'rb') as f:\n",
    "        trajectories = pickle.load(f)\n",
    "    # with open(path+'/test_trajectory.pkl.gz', 'rb') as f:\n",
    "    #     trajectories = pickle.load(f)\n",
    "\n",
    "    # row = []\n",
    "    # row.append(episode_count+1)\n",
    "    # row.append(episode_step)\n",
    "    # for agent in env.world.agents:\n",
    "    #     row.append(agent.state.p_pos[0])\n",
    "    #     row.append(agent.state.p_pos[1])\n",
    "    # for landmark in env.world.landmarks:\n",
    "    #     row.append(landmark.state.p_pos[0])\n",
    "    #     row.append(landmark.state.p_pos[1])\n",
    "    # row.append(rew_n[0])\n",
    "    # row.append(rew_n[1])\n",
    "    # if arglist.num_agents == 3:\n",
    "    #     row.append(rew_n[2])\n",
    "    # episode_trajectory.append(row)\n",
    "\n",
    "    data = []\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for traj in trajectories:\n",
    "        list_episode = []\n",
    "        list_timestep = []\n",
    "        list_agent1_x = []\n",
    "        list_agent1_y = []\n",
    "        list_agent2_x = []\n",
    "        list_agent2_y = []\n",
    "        list_agent3_x = []\n",
    "        list_agent3_y = []\n",
    "        list_land1_x = []\n",
    "        list_land1_y = []\n",
    "        list_land2_x = []\n",
    "        list_land2_y = []\n",
    "        list_land3_x = []\n",
    "        list_land3_y = []\n",
    "        list_reward_1 = []\n",
    "        list_reward_2 = []\n",
    "        list_reward_3 = []\n",
    "        list_agent1_cum_reward = []\n",
    "        list_agent2_cum_reward = []\n",
    "        list_agent3_cum_reward = []\n",
    "        list_agent1_dist = []\n",
    "        list_agent2_dist = []\n",
    "        list_agent3_dist = []\n",
    "        cum_rew_1 = 0\n",
    "        cum_rew_2 = 0\n",
    "        cum_rew_3 = 0\n",
    "        for row in traj:\n",
    "                if num_agents == 3:\n",
    "                    list_episode.append(row[0])\n",
    "                    list_timestep.append(row[1])\n",
    "                    list_agent1_x.append(row[2])\n",
    "                    list_agent1_y.append(row[3])\n",
    "                    list_agent2_x.append(row[4])\n",
    "                    list_agent2_y.append(row[5])\n",
    "                    list_agent3_x.append(row[6])\n",
    "                    list_agent3_y.append(row[7])\n",
    "                    list_agents_x = [row[2], row[4], row[6]]\n",
    "                    list_agents_y = [row[3], row[5], row[7]]\n",
    "                    list_land1_x.append(row[8])\n",
    "                    list_land1_y.append(row[9])\n",
    "                    list_land2_x.append(row[10])\n",
    "                    list_land2_y.append(row[11])\n",
    "                    list_land3_x.append(row[12])\n",
    "                    list_land3_y.append(row[13])\n",
    "                    list_landmarks_x = [row[8], row[10], row[12]]\n",
    "                    list_landmarks_y = [row[9], row[11], row[13]]\n",
    "                    list_reward_1.append(row[14])\n",
    "                    list_reward_2.append(row[15])\n",
    "                    list_reward_3.append(row[16])\n",
    "                if num_agents == 2:\n",
    "                    list_episode.append(row[0])\n",
    "                    list_timestep.append(row[1])\n",
    "                    list_agent1_x.append(row[2])\n",
    "                    list_agent1_y.append(row[3])\n",
    "                    list_agent2_x.append(row[4])\n",
    "                    list_agent2_y.append(row[5])\n",
    "                    list_agents_x = [row[2], row[4]]\n",
    "                    list_agents_y = [row[3], row[5]]\n",
    "                    list_land1_x.append(row[6])\n",
    "                    list_land1_y.append(row[7])\n",
    "                    list_land2_x.append(row[8])\n",
    "                    list_land2_y.append(row[9])\n",
    "                    list_landmarks_x = [row[6], row[8]]\n",
    "                    list_landmarks_y = [row[7], row[9]]\n",
    "                    list_reward_1.append(row[10])\n",
    "                    list_reward_2.append(row[11])\n",
    "\n",
    "\n",
    "                dists = []\n",
    "                for agent_x, agent_y in zip(list_agents_x, list_agents_y):\n",
    "                    dist = []\n",
    "                    for land_x, land_y in zip(list_landmarks_x, list_landmarks_y):\n",
    "                        dist.append(np.linalg.norm(np.array([agent_x, agent_y]) - np.array([land_x, land_y])))\n",
    "                    dists.append(min(dist))\n",
    "                if num_agents == 3:\n",
    "                    cum_rew_1 += row[14]\n",
    "                    cum_rew_2 += row[15]\n",
    "                    cum_rew_3 += row[16]\n",
    "                if num_agents == 2:\n",
    "                    cum_rew_1 += row[10]\n",
    "                    cum_rew_2 += row[11]\n",
    "                list_agent1_cum_reward.append(cum_rew_1)\n",
    "                list_agent2_cum_reward.append(cum_rew_2)\n",
    "                if num_agents == 3:\n",
    "                    list_agent3_cum_reward.append(cum_rew_3)\n",
    "                list_agent1_dist.append(dists[0])\n",
    "                list_agent2_dist.append(dists[1])\n",
    "                if num_agents == 3:\n",
    "                    list_agent3_dist.append(dists[2])\n",
    "\n",
    "\n",
    "        trajectory_ = {'episode': list_episode,\n",
    "                       'timestep': list_timestep,\n",
    "                          'agent1_x': list_agent1_x,\n",
    "                            'agent1_y': list_agent1_y,\n",
    "                            'agent2_x': list_agent2_x,\n",
    "                            'agent2_y': list_agent2_y,\n",
    "                            'agent3_x': list_agent3_x,\n",
    "                            'agent3_y': list_agent3_y,\n",
    "                            'land1_x': list_land1_x,\n",
    "                            'land1_y': list_land1_y,\n",
    "                            'land2_x': list_land2_x,\n",
    "                            'land2_y': list_land2_y,\n",
    "                            'land3_x': list_land3_x,\n",
    "                            'land3_y': list_land3_y,\n",
    "                            'reward_1': list_reward_1,\n",
    "                            'reward_2': list_reward_2,\n",
    "                            'reward_3': list_reward_3,\n",
    "                            'agent1_cum_reward': list_agent1_cum_reward,\n",
    "                            'agent2_cum_reward': list_agent2_cum_reward,\n",
    "                            'agent3_cum_reward': list_agent3_cum_reward,\n",
    "                            'agent1_dist': list_agent1_dist,\n",
    "                            'agent2_dist': list_agent2_dist,\n",
    "                            'agent3_dist': list_agent3_dist\n",
    "                       }\n",
    "        if num_agents == 2:\n",
    "            del trajectory_['agent3_x']\n",
    "            del trajectory_['agent3_y']\n",
    "            del trajectory_['land3_x']\n",
    "            del trajectory_['land3_y']\n",
    "            del trajectory_['reward_3']\n",
    "            del trajectory_['agent3_cum_reward']\n",
    "            del trajectory_['agent3_dist']\n",
    "        data.append(trajectory_)\n",
    "        \n",
    "    # Initialize empty lists for aggregation\n",
    "    aggregated_data = {key: [] for key in data[0].keys()}\n",
    "\n",
    "    # Aggregate corresponding elements\n",
    "    for d in data:\n",
    "        for key in d:\n",
    "            aggregated_data[key].extend(d[key])\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(aggregated_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # calculate cumulative reward for each agent until each timestep as additional columns\n",
    "    # for trajectory in trajectories:\n",
    "    #     for i in range(len(trajectory)):\n",
    "    #         trajectory[i].extend([sum([row[14] for row in trajectory[:i+1]]), sum([row[15] for row in trajectory[:i+1]]), sum([row[16] for row in trajectory[:i+1]])])\n",
    "\n",
    "    # for trajectory in trajectories:\n",
    "    #     for k in range(len(trajectory)):\n",
    "    #         dists = []\n",
    "    #         for i in range(3):\n",
    "    #             dist = []\n",
    "    #             for j in range(3):\n",
    "    #                 dist.append(np.linalg.norm(np.array(trajectory[k][2+2*i:4+2*i]) - np.array(trajectory[k][8+2*j:10+2*j])))\n",
    "    #             dists.append(min(dist))\n",
    "    #         trajectory[k].extend(dists)\n",
    "\n",
    "    # trajectories is a list of trajectories. Where each trajectory is a list of:\n",
    "    # [timestep, agent1_x, agent1_y, agent2_x, agent2_y, agent3_x, agent3_y, landmark1_x, landmark1_y, landmark2_x, landmark2_y, landmark3_x, landmark3_y, reward_1, reward_2, reward_3]\n",
    "    # convert trajectories to pd dataframe with columns: episode, timestep, agent1_x, agent1_y, agent2_x, agent2_y, agent3_x, agent3_y, reward_1, reward_2, reward_3\n",
    "    # Flatten the nested list\n",
    "    # flattened_data = [tup for trajectory in trajectories for tup in trajectory]\n",
    "    # # Convert to DataFrame\n",
    "    # df = pd.DataFrame(flattened_data, columns=[ 'episode', 'timestep',\n",
    "    #                                             'agent1_x', 'agent1_y',\n",
    "    #                                             'agent2_x', 'agent2_y', \n",
    "    #                                             'agent3_x', 'agent3_y', \n",
    "    #                                             'land1_x', 'land1_y',\n",
    "    #                                             'land2_x', 'land2_y', \n",
    "    #                                             'land3_x', 'land3_y', \n",
    "    #                                             'reward_1', 'reward_2', 'reward_3',\n",
    "    #                                             'agent1_cum_reward', 'agent2_cum_reward', 'agent3_cum_reward',\n",
    "    #                                             'agent1_dist', 'agent2_dist', 'agent3_dist'])\n",
    "    # df = pd.DataFrame()\n",
    "    return df, data\n",
    "\n",
    "df, trajectories = load_data(run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Performance Analysis\n",
    "This code runs analysis on all the test episodes wihin the loaded run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#W4sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39m# fig.savefig(run_name+'_analysis.png', dpi=300)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#W4sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#W4sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m plot_test_analysis(df, run_name)\n",
      "\u001b[1;32m/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# sns.violinplot(data=df, x=\"timestep\", y=\"reward_1\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m melted_df \u001b[39m=\u001b[39m data\u001b[39m=\u001b[39mdf[df[\u001b[39m'\u001b[39m\u001b[39mtimestep\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m70\u001b[39m]\u001b[39m.\u001b[39mmelt(value_vars\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39magent1_cum_reward\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39magent2_cum_reward\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39magent3_cum_reward\u001b[39m\u001b[39m'\u001b[39m], var_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39magent_name\u001b[39m\u001b[39m'\u001b[39m, value_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m sns\u001b[39m.\u001b[39;49mhistplot(ax\u001b[39m=\u001b[39;49maxs[\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m], data\u001b[39m=\u001b[39;49mmelted_df, x\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m'\u001b[39;49m, hue\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39magent_name\u001b[39;49m\u001b[39m'\u001b[39;49m, element\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mstep\u001b[39;49m\u001b[39m'\u001b[39;49m, common_norm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, bins\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m axs[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mcumulative shared reward\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m axs[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_ylabel(\u001b[39m'\u001b[39m\u001b[39mnumber of episodes\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:1416\u001b[0m, in \u001b[0;36mhistplot\u001b[0;34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m   1405\u001b[0m estimate_kws \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   1406\u001b[0m     stat\u001b[39m=\u001b[39mstat,\n\u001b[1;32m   1407\u001b[0m     bins\u001b[39m=\u001b[39mbins,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1411\u001b[0m     cumulative\u001b[39m=\u001b[39mcumulative,\n\u001b[1;32m   1412\u001b[0m )\n\u001b[1;32m   1414\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39munivariate:\n\u001b[0;32m-> 1416\u001b[0m     p\u001b[39m.\u001b[39;49mplot_univariate_histogram(\n\u001b[1;32m   1417\u001b[0m         multiple\u001b[39m=\u001b[39;49mmultiple,\n\u001b[1;32m   1418\u001b[0m         element\u001b[39m=\u001b[39;49melement,\n\u001b[1;32m   1419\u001b[0m         fill\u001b[39m=\u001b[39;49mfill,\n\u001b[1;32m   1420\u001b[0m         shrink\u001b[39m=\u001b[39;49mshrink,\n\u001b[1;32m   1421\u001b[0m         common_norm\u001b[39m=\u001b[39;49mcommon_norm,\n\u001b[1;32m   1422\u001b[0m         common_bins\u001b[39m=\u001b[39;49mcommon_bins,\n\u001b[1;32m   1423\u001b[0m         kde\u001b[39m=\u001b[39;49mkde,\n\u001b[1;32m   1424\u001b[0m         kde_kws\u001b[39m=\u001b[39;49mkde_kws,\n\u001b[1;32m   1425\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[1;32m   1426\u001b[0m         legend\u001b[39m=\u001b[39;49mlegend,\n\u001b[1;32m   1427\u001b[0m         estimate_kws\u001b[39m=\u001b[39;49mestimate_kws,\n\u001b[1;32m   1428\u001b[0m         line_kws\u001b[39m=\u001b[39;49mline_kws,\n\u001b[1;32m   1429\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1430\u001b[0m     )\n\u001b[1;32m   1432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     p\u001b[39m.\u001b[39mplot_bivariate_histogram(\n\u001b[1;32m   1435\u001b[0m         common_bins\u001b[39m=\u001b[39mcommon_bins,\n\u001b[1;32m   1436\u001b[0m         common_norm\u001b[39m=\u001b[39mcommon_norm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   1447\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:456\u001b[0m, in \u001b[0;36m_DistributionPlotter.plot_univariate_histogram\u001b[0;34m(self, multiple, element, fill, common_norm, common_bins, shrink, kde, kde_kws, color, legend, line_kws, estimate_kws, **plot_kws)\u001b[0m\n\u001b[1;32m    447\u001b[0m     densities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_univariate_density(\n\u001b[1;32m    448\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_variable,\n\u001b[1;32m    449\u001b[0m         common_norm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         warn_singular\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    453\u001b[0m     )\n\u001b[1;32m    455\u001b[0m \u001b[39m# First pass through the data to compute the histograms\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m \u001b[39mfor\u001b[39;00m sub_vars, sub_data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_data(\u001b[39m\"\u001b[39m\u001b[39mhue\u001b[39m\u001b[39m\"\u001b[39m, from_comp_data\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    457\u001b[0m \n\u001b[1;32m    458\u001b[0m     \u001b[39m# Prepare the relevant data\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(sub_vars\u001b[39m.\u001b[39mitems())\n\u001b[1;32m    460\u001b[0m     orient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_variable\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/_base.py:938\u001b[0m, in \u001b[0;36mVectorPlotter.iter_data\u001b[0;34m(self, grouping_vars, reverse, from_comp_data, by_facet, allow_empty, dropna)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m grouping_vars:\n\u001b[1;32m    936\u001b[0m     grouping_keys\u001b[39m.\u001b[39mappend(levels\u001b[39m.\u001b[39mget(var, []))\n\u001b[0;32m--> 938\u001b[0m iter_keys \u001b[39m=\u001b[39m itertools\u001b[39m.\u001b[39;49mproduct(\u001b[39m*\u001b[39;49mgrouping_keys)\n\u001b[1;32m    939\u001b[0m \u001b[39mif\u001b[39;00m reverse:\n\u001b[1;32m    940\u001b[0m     iter_keys \u001b[39m=\u001b[39m \u001b[39mreversed\u001b[39m(\u001b[39mlist\u001b[39m(iter_keys))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "def plot_test_analysis(df, run_name):\n",
    "    %matplotlib qt\n",
    "    # Colors\n",
    "    BLUE = [0, 0.4470, 0.7410]\n",
    "    RED = [0.8500, 0.3250, 0.0980]\n",
    "    YELLOW = [0.929, 0.6940, 0.1250]\n",
    "\n",
    "    p = [BLUE, RED, YELLOW]\n",
    "    sns.set_palette(p)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(2,3, figsize=(15,8))\n",
    "\n",
    "    fig.suptitle('Policy used: {}'.format(run_name), fontsize=16)\n",
    "\n",
    "\n",
    "    sns.lineplot(ax=axs[0,0], x=\"timestep\", y=\"agent1_dist\", data=df, errorbar=(\"sd\",1), sort=False, color=BLUE, label='agent 1')\n",
    "    sns.lineplot(ax=axs[0,0], x=\"timestep\", y=\"agent2_dist\", data=df, errorbar=(\"sd\",1), sort=False, color=RED, label='agent 2')\n",
    "    sns.lineplot(ax=axs[0,0], x=\"timestep\", y=\"agent3_dist\", data=df, errorbar=(\"sd\",1), sort=False, color=YELLOW, label='agent 3')\n",
    "    axs[0,0].set_xlabel('timestep')\n",
    "    axs[0,0].set_ylabel('distance to closest landmark')\n",
    "    axs[0,0].grid()\n",
    "\n",
    "    # plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\n",
    "    # sns.violinplot(data=df, x=\"timestep\", y=\"reward_1\")\n",
    "    melted_df = data=df[df['timestep']==70].melt(value_vars=['agent1_cum_reward', 'agent2_cum_reward', 'agent3_cum_reward'], var_name='agent_name', value_name='value')\n",
    "    sns.histplot(ax=axs[0,1], data=melted_df, x='value', hue='agent_name', element='step', common_norm=False, bins=30)\n",
    "    axs[0,1].set_xlabel('cumulative shared reward')\n",
    "    axs[0,1].set_ylabel('number of episodes')\n",
    "    axs[0,1].legend(['agent 1', 'agent 2', 'agent 3'])\n",
    "\n",
    "\n",
    "    sns.lineplot(ax=axs[0,2], x=\"timestep\", y=\"agent1_cum_reward\", data=df, errorbar=(\"sd\",1), sort=False, color=BLUE, label='agent 1')\n",
    "    sns.lineplot(ax=axs[0,2], x=\"timestep\", y=\"agent2_cum_reward\", data=df, errorbar=(\"sd\",1), sort=False, color=RED, label='agent 2')\n",
    "    sns.lineplot(ax=axs[0,2], x=\"timestep\", y=\"agent3_cum_reward\", data=df, errorbar=(\"sd\",1), sort=False, color=YELLOW, label='agent 3')\n",
    "    axs[0,2].set_xlabel('timestep')\n",
    "    axs[0,2].set_ylabel('cumulative shared reward')\n",
    "    axs[0,2].grid()\n",
    "\n",
    "    # plt.subplot(2,2,4)\n",
    "    sns.lineplot(ax=axs[1,0], x=\"timestep\", y=\"reward_1\", data=df, errorbar=(\"sd\",1), sort=False, color=BLUE, label='agent 1')\n",
    "    sns.lineplot(ax=axs[1,0], x=\"timestep\", y=\"reward_2\", data=df, errorbar=(\"sd\",1), sort=False, color=RED, label='agent 2')\n",
    "    sns.lineplot(ax=axs[1,0], x=\"timestep\", y=\"reward_3\", data=df, errorbar=(\"sd\",1), sort=False, color=YELLOW, label='agent 3')\n",
    "    axs[1,0].set_xlabel('timestep')\n",
    "    axs[1,0].set_ylabel('mean shared reward')\n",
    "    axs[1,0].grid()\n",
    "\n",
    "\n",
    "    # plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\n",
    "    melted_df = data=df[df['timestep']==1].melt(value_vars=['agent1_dist', 'agent2_dist', 'agent3_dist'], var_name='agent_name', value_name='value')\n",
    "    sns.histplot(ax=axs[1,1],data=melted_df, x='value', hue='agent_name', element='step', common_norm=False, bins=30)\n",
    "    axs[1,1].set_xlabel('distance to closest landmark at the first timestep')\n",
    "    axs[1,1].set_ylabel('number of episodes')\n",
    "    axs[1,1].legend(['agent 1', 'agent 2', 'agent 3'])\n",
    "\n",
    "\n",
    "    # plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\n",
    "    melted_df = data=df[df['timestep']==70].melt(value_vars=['agent1_dist', 'agent2_dist', 'agent3_dist'], var_name='agent_name', value_name='value')\n",
    "    sns.histplot(ax=axs[1,2], data=melted_df, x='value', hue='agent_name', element='step', common_norm=False, bins=30)\n",
    "    axs[1,2].set_xlabel('distance to closest landmark at the last timestep')\n",
    "    axs[1,2].set_ylabel('number of episodes')\n",
    "    axs[1,2].legend(['agent 1', 'agent 2', 'agent 3'])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # fig.savefig(run_name+'_analysis.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_test_analysis(df, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic Analysis\n",
    "This code plots the details of a particular episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "episode = 90\n",
    "\n",
    "plt.figure(figsize=(15,4.5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "# plot the x,y trajectory of agents in episode 1 sorted by timestep using seaborn\n",
    "sns.lineplot(x=\"agent1_x\", y=\"agent1_y\", data=df[df['episode']==episode], sort=False, color=BLUE)\n",
    "sns.lineplot(x=\"agent2_x\", y=\"agent2_y\", data=df[df['episode']==episode], sort=False, color=RED)\n",
    "sns.lineplot(x=\"agent3_x\", y=\"agent3_y\", data=df[df['episode']==episode], sort=False, color=YELLOW)\n",
    "# mark the location of landmarks\n",
    "sns.scatterplot(x=\"land1_x\", y=\"land1_y\", data=df[df['episode']==episode], color='gray', s=300)\n",
    "sns.scatterplot(x=\"land2_x\", y=\"land2_y\", data=df[df['episode']==episode], color='gray', s=300)\n",
    "sns.scatterplot(x=\"land3_x\", y=\"land3_y\", data=df[df['episode']==episode], color='gray', s=300)\n",
    "# make axis equal and limit the axis to -1.0 to 1.0\n",
    "plt.axis('square')\n",
    "plt.xlim(-1.5,1.5)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "# plot the reward of agents in episode 1 sorted by timestep using seaborn\n",
    "sns.lineplot(x=\"timestep\", y=\"reward_1\", data=df[df['episode']==episode], sort=False, color=BLUE, label='agent1')\n",
    "sns.lineplot(x=\"timestep\", y=\"reward_2\", data=df[df['episode']==episode], sort=False, color=RED, label='agent2')\n",
    "sns.lineplot(x=\"timestep\", y=\"reward_3\", data=df[df['episode']==episode], sort=False, color=YELLOW, label='agent3')\n",
    "plt.xlabel('timestep')\n",
    "plt.ylabel('shared reward')\n",
    "plt.title('Episode {}'.format(episode))\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "# plot the distance to the closest landmark for each agent\n",
    "sns.lineplot(x=\"timestep\", y=\"agent1_dist\", data=df[df['episode']==episode], sort=False, color=BLUE, label='agent1')\n",
    "sns.lineplot(x=\"timestep\", y=\"agent2_dist\", data=df[df['episode']==episode], sort=False, color=RED, label='agent2')\n",
    "sns.lineplot(x=\"timestep\", y=\"agent3_dist\", data=df[df['episode']==episode], sort=False, color=YELLOW, label='agent3')\n",
    "plt.xlabel('timestep')\n",
    "plt.ylabel('distance to closest landmark')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     ax\u001b[39m.\u001b[39mset_aspect(\u001b[39m'\u001b[39m\u001b[39mequal\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m plot_reward_function(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def reward_function(d):\n",
    "        return np.exp(-(d**2)/.1)\n",
    "\n",
    "def plot_reward_function(df):\n",
    "\n",
    "    # get the landmark locations for episode 1 timestep 1 from df\n",
    "    landmark_locations = df[(df['timestep']==1) & (df['episode']==1)][['land1_x', 'land1_y', 'land2_x', 'land2_y', 'land3_x', 'land3_y']]\n",
    "    x = np.linspace(-1, 1, 100)\n",
    "    y = np.linspace(-1, 1, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    lanmark_locations = [(landmark_locations.land1_x.values[0], landmark_locations.land1_y.values[0]), \n",
    "                         (landmark_locations.land2_x.values[0], landmark_locations.land2_y.values[0]), \n",
    "                         (landmark_locations.land3_x.values[0], landmark_locations.land3_y.values[0])]\n",
    "\n",
    "    # calculate the distance to the closest landmark for each point in the grid\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(Y)):\n",
    "            dists = []\n",
    "            for k in range(3):\n",
    "                dists.append(np.linalg.norm(np.array([X[i,j], Y[i,j]]) - np.array(lanmark_locations[k])))\n",
    "            Z[i,j] = reward_function(min(dists))\n",
    "    # plot the reward function as a 2D heatmap using sns\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('Reward Function')\n",
    "    cs = ax.contourf(X, Y, Z, 50, cmap='cividis')\n",
    "    cbar = fig.colorbar(cs)\n",
    "    # limit the colorbar to 0 to 1 and make sure 0 and 1 are part of the ticks\n",
    "    cbar.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    # plot contour lines with labels on top of the heatmap\n",
    "    cs = ax.contour(X, Y, Z, [0.01,0.1,0.25,0.5,0.75,0.95], colors='white', linewidths=0.5)\n",
    "    # add specific contour values\n",
    "    ax.clabel(cs, inline=1, fontsize=8)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.show()\n",
    "plot_reward_function(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the episode number of the wrost performing episodes and see the behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>timestep</th>\n",
       "      <th>agent1_x</th>\n",
       "      <th>agent1_y</th>\n",
       "      <th>agent2_x</th>\n",
       "      <th>agent2_y</th>\n",
       "      <th>agent3_x</th>\n",
       "      <th>agent3_y</th>\n",
       "      <th>land1_x</th>\n",
       "      <th>land1_y</th>\n",
       "      <th>...</th>\n",
       "      <th>land3_y</th>\n",
       "      <th>reward_1</th>\n",
       "      <th>reward_2</th>\n",
       "      <th>reward_3</th>\n",
       "      <th>agent1_cum_reward</th>\n",
       "      <th>agent2_cum_reward</th>\n",
       "      <th>agent3_cum_reward</th>\n",
       "      <th>agent1_dist</th>\n",
       "      <th>agent2_dist</th>\n",
       "      <th>agent3_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58379</th>\n",
       "      <td>834</td>\n",
       "      <td>70</td>\n",
       "      <td>3.719370</td>\n",
       "      <td>-2.282283</td>\n",
       "      <td>-0.229380</td>\n",
       "      <td>0.396079</td>\n",
       "      <td>0.719549</td>\n",
       "      <td>0.033549</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>1.395175e-68</td>\n",
       "      <td>1.395175e-68</td>\n",
       "      <td>1.395175e-68</td>\n",
       "      <td>2.300226e-10</td>\n",
       "      <td>2.300226e-10</td>\n",
       "      <td>2.300226e-10</td>\n",
       "      <td>3.946284</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.222097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>4.470279</td>\n",
       "      <td>-1.845780</td>\n",
       "      <td>7.713436</td>\n",
       "      <td>-0.558476</td>\n",
       "      <td>-1.034868</td>\n",
       "      <td>-1.584986</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>9.401575e-320</td>\n",
       "      <td>9.401575e-320</td>\n",
       "      <td>9.401575e-320</td>\n",
       "      <td>6.666467e-10</td>\n",
       "      <td>6.666467e-10</td>\n",
       "      <td>6.666467e-10</td>\n",
       "      <td>4.378358</td>\n",
       "      <td>7.235023</td>\n",
       "      <td>1.393937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14209</th>\n",
       "      <td>203</td>\n",
       "      <td>70</td>\n",
       "      <td>4.072829</td>\n",
       "      <td>0.775028</td>\n",
       "      <td>6.683566</td>\n",
       "      <td>-1.780517</td>\n",
       "      <td>0.915880</td>\n",
       "      <td>-0.981435</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>1.554979e-243</td>\n",
       "      <td>1.554979e-243</td>\n",
       "      <td>1.554979e-243</td>\n",
       "      <td>7.240644e-10</td>\n",
       "      <td>7.240644e-10</td>\n",
       "      <td>7.240644e-10</td>\n",
       "      <td>3.655923</td>\n",
       "      <td>6.434806</td>\n",
       "      <td>1.065913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47529</th>\n",
       "      <td>679</td>\n",
       "      <td>70</td>\n",
       "      <td>4.398247</td>\n",
       "      <td>0.471654</td>\n",
       "      <td>6.204931</td>\n",
       "      <td>5.244140</td>\n",
       "      <td>0.940527</td>\n",
       "      <td>-0.545348</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.760432e-09</td>\n",
       "      <td>1.760432e-09</td>\n",
       "      <td>1.760432e-09</td>\n",
       "      <td>3.926676</td>\n",
       "      <td>7.749016</td>\n",
       "      <td>0.701048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>72</td>\n",
       "      <td>70</td>\n",
       "      <td>2.448700</td>\n",
       "      <td>-0.335249</td>\n",
       "      <td>1.154482</td>\n",
       "      <td>2.644794</td>\n",
       "      <td>0.475252</td>\n",
       "      <td>-0.664066</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>1.948412e-49</td>\n",
       "      <td>1.948412e-49</td>\n",
       "      <td>1.948412e-49</td>\n",
       "      <td>3.097751e-09</td>\n",
       "      <td>3.097751e-09</td>\n",
       "      <td>3.097751e-09</td>\n",
       "      <td>1.977328</td>\n",
       "      <td>2.620028</td>\n",
       "      <td>0.664527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode  timestep  agent1_x  agent1_y  agent2_x  agent2_y  agent3_x  \\\n",
       "58379      834        70  3.719370 -2.282283 -0.229380  0.396079  0.719549   \n",
       "5319        76        70  4.470279 -1.845780  7.713436 -0.558476 -1.034868   \n",
       "14209      203        70  4.072829  0.775028  6.683566 -1.780517  0.915880   \n",
       "47529      679        70  4.398247  0.471654  6.204931  5.244140  0.940527   \n",
       "5039        72        70  2.448700 -0.335249  1.154482  2.644794  0.475252   \n",
       "\n",
       "       agent3_y  land1_x  land1_y  ...   land3_y       reward_1  \\\n",
       "58379  0.033549      0.5      0.0  ... -0.433013   1.395175e-68   \n",
       "5319  -1.584986      0.5      0.0  ... -0.433013  9.401575e-320   \n",
       "14209 -0.981435      0.5      0.0  ... -0.433013  1.554979e-243   \n",
       "47529 -0.545348      0.5      0.0  ... -0.433013   0.000000e+00   \n",
       "5039  -0.664066      0.5      0.0  ... -0.433013   1.948412e-49   \n",
       "\n",
       "            reward_2       reward_3  agent1_cum_reward  agent2_cum_reward  \\\n",
       "58379   1.395175e-68   1.395175e-68       2.300226e-10       2.300226e-10   \n",
       "5319   9.401575e-320  9.401575e-320       6.666467e-10       6.666467e-10   \n",
       "14209  1.554979e-243  1.554979e-243       7.240644e-10       7.240644e-10   \n",
       "47529   0.000000e+00   0.000000e+00       1.760432e-09       1.760432e-09   \n",
       "5039    1.948412e-49   1.948412e-49       3.097751e-09       3.097751e-09   \n",
       "\n",
       "       agent3_cum_reward  agent1_dist  agent2_dist  agent3_dist  \n",
       "58379       2.300226e-10     3.946284     0.042300     0.222097  \n",
       "5319        6.666467e-10     4.378358     7.235023     1.393937  \n",
       "14209       7.240644e-10     3.655923     6.434806     1.065913  \n",
       "47529       1.760432e-09     3.926676     7.749016     0.701048  \n",
       "5039        3.097751e-09     1.977328     2.620028     0.664527  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the episodes with the worst cumulative reward at the timestep 70\n",
    "df[df['timestep']==70].sort_values(by='agent1_cum_reward', ascending=True).head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Reply Animation\n",
    "\n",
    "Just adjust the episode number and observe the behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now showing episode 2\n",
      "Now showing episode 3\n",
      "Now showing episode 4\n",
      "Now showing episode 5\n",
      "Now showing episode 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X16sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m     pygame\u001b[39m.\u001b[39mquit()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X16sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X16sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m     main()\n",
      "\u001b[1;32m/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X16sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     \u001b[39mif\u001b[39;00m event\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m pygame\u001b[39m.\u001b[39mQUIT:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X16sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m         running \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X16sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m clock\u001b[39m.\u001b[39;49mtick(\u001b[39m100\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X16sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39m# Update current time\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hossein/repos/multi-agent-rsrn/maddpg/experiments/visualize_test_3-agent.ipynb#X16sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m current_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m.1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import pygame\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# drop the first element of the list for each list item in trajectory\n",
    "# trajectory_ = [t[1:] for t in trajectory_]\n",
    "\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "# Display settings\n",
    "WIDTH, HEIGHT = 640, 480\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Multi-Agent and Landmarks Animation\")\n",
    "\n",
    "# Colors\n",
    "BLUE = np.array([0, 0.4470*255, 0.7410*255])\n",
    "RED = np.array([0.8500*255, 0.3250*255, 0.0980*255])\n",
    "YELLOW = np.array([0.929*255, 0.6940*255, 0.1250*255])\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "AGENTS_COLORS = [BLUE, RED, YELLOW]\n",
    "LANDMARK_COLORS = [(50, 50, 50), (50, 50, 50), (50, 50, 50)]\n",
    "BLACK = (20, 20, 20)\n",
    "\n",
    "frames = []\n",
    "# shift and scale the data (-1,1) to fit the screen size (0,500) and (0,500)\n",
    "SCALE_FACTOR = 200  # given our dimensions and trajectory range\n",
    "SCREEN_CENTER = (WIDTH // 2, HEIGHT // 2)\n",
    "\n",
    "# Choose a font (using a default system font here)\n",
    "font = pygame.font.SysFont(\"arial\", 16)\n",
    "\n",
    "def map_to_screen(pos):\n",
    "    \"\"\"Map a trajectory position to a screen position.\"\"\"\n",
    "    return int(pos[0] * SCALE_FACTOR + SCREEN_CENTER[0]), int(pos[1] * SCALE_FACTOR + SCREEN_CENTER[1])\n",
    "\n",
    "\n",
    "def draw_entity(screen, x, y, color, size=.2*SCALE_FACTOR):\n",
    "    pygame.draw.circle(screen, color, (int(x), int(y)), size)\n",
    "\n",
    "\n",
    "\n",
    "def display_text(text, x, y, color=BLACK):\n",
    "    \"\"\"Render and display text on the screen at specified coordinates.\"\"\"\n",
    "    text_surface = font.render(text, True, color)\n",
    "    screen.blit(text_surface, (x, y))\n",
    "\n",
    "def capture_frame(screen):\n",
    "    \"\"\"Capture the current Pygame screen frame.\"\"\"\n",
    "    frame = pygame.Surface(screen.get_size())\n",
    "    frame.blit(screen, (0, 0))\n",
    "    frames.append(frame)\n",
    "\n",
    "\n",
    "def save_frames_to_video(frames, filename, fps=30):\n",
    "    \"\"\"Save captured frames to a video file.\"\"\"\n",
    "    height, width = frames[0].get_size()\n",
    "    size = (width, height)\n",
    "    out = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'XVID'), fps, size)\n",
    "    for frame in frames:\n",
    "        # Convert Pygame surface to OpenCV format\n",
    "        frame_rgb = pygame.surfarray.array3d(frame).transpose([1, 0, 2])\n",
    "        frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "def main():\n",
    "    episode = 1\n",
    "    # Sample trajectory data\n",
    "    trajectory_ = trajectories[episode]\n",
    "    time.sleep(10)\n",
    "    clock = pygame.time.Clock()\n",
    "    running = True\n",
    "    current_time = 0\n",
    "    while running:\n",
    "        screen.fill(WHITE)\n",
    "        # Calculate the current positions of the agents and landmarks\n",
    "        for i in range(69):\n",
    "\n",
    "            cum_rew_1 = trajectory_['agent1_cum_reward'][i]\n",
    "            cum_rew_2 = trajectory_['agent2_cum_reward'][i]\n",
    "            if num_agents == 3:\n",
    "                cum_rew_3 = trajectory_['agent3_cum_reward'][i]\n",
    "\n",
    "            t0 = trajectory_['timestep'][i]\n",
    "            t1 = trajectory_['timestep'][i+1]\n",
    "            \n",
    "            if t0 <= current_time < t1:\n",
    "                # clear_output(wait=True)\n",
    "                # print(current_time, t0, t1)\n",
    "                alpha = (current_time - t0) / (t1 - t0)\n",
    "\n",
    "                # Drawing agents\n",
    "                for j in range(num_agents):  \n",
    "                    x0, y0 = trajectory_['agent{}_x'.format(j+1)][i], trajectory_['agent{}_y'.format(j+1)][i]\n",
    "                    x1, y1 = trajectory_['agent{}_x'.format(j+1)][i+1], trajectory_['agent{}_y'.format(j+1)][i+1]\n",
    "                    # r0, r1 = data0[12 + j], data1[12 + j]\n",
    "                    x = x0 * (1 - alpha) + x1 * alpha\n",
    "                    y = y0 * (1 - alpha) + y1 * alpha\n",
    "                    r = trajectory_['reward_{}'.format(j+1)][i]\n",
    "                    screen_x, screen_y = map_to_screen((x, y))\n",
    "                    draw_entity(screen, screen_x, screen_y, AGENTS_COLORS[j])\n",
    "                    display_text(f\"{r:.4f}\", screen_x-25, screen_y-60)\n",
    "\n",
    "                # Drawing landmarks\n",
    "                for j in range(num_agents):\n",
    "                    x0, y0 = trajectory_['land{}_x'.format(j+1)][i], trajectory_['land{}_y'.format(j+1)][i]\n",
    "                    x1, y1 = trajectory_['land{}_x'.format(j+1)][i+1], trajectory_['land{}_y'.format(j+1)][i+1]\n",
    "                    x = x0 * (1 - alpha) + x1 * alpha\n",
    "                    y = y0 * (1 - alpha) + y1 * alpha\n",
    "                    screen_x, screen_y = map_to_screen((x, y))\n",
    "                    draw_entity(screen, screen_x, screen_y, LANDMARK_COLORS[j], size=.05*SCALE_FACTOR)\n",
    "\n",
    "                display_text(f\"agent 1 cumulative reward: {cum_rew_1:.2f}\", 370, 10)\n",
    "                display_text(f\"agent 2 cumulative reward: {cum_rew_2:.2f}\", 370, 30)\n",
    "                if num_agents == 3:\n",
    "                    display_text(f\"agent 3 cumulative reward: {cum_rew_3:.2f}\", 370, 50)\n",
    "                display_text(f\"episode: {episode}\",10, 20)\n",
    "                display_text(f\"timestep: {trajectory_['timestep'][i]}\", 120, 20)\n",
    "            # else:\n",
    "                # print('not in range', current_time, t0, t1)\n",
    "        pygame.display.flip()   \n",
    "        capture_frame(screen)\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "        \n",
    "        clock.tick(100)\n",
    "        # Update current time\n",
    "        current_time += .1\n",
    "\n",
    "        # Exit loop when trajectory ends\n",
    "        if current_time >= 69:\n",
    "            print('Now showing episode {}'.format(episode+1))\n",
    "            episode += 1\n",
    "            trajectory_ = trajectories[episode]\n",
    "            current_time = 0\n",
    "            # running = False\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # save_frames_to_video(frames,'episode_{}.avi'.format(episode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "def WSM(r1, r2):\n",
    "    return r1 + r2\n",
    "\n",
    "def WPM(r1, r2):\n",
    "    return r1 * r2\n",
    "\n",
    "def MinMax(r1, r2):\n",
    "    return np.minimum(r1, r2)\n",
    "\n",
    "def plot_shared_reward_function():\n",
    "    r1 = np.linspace(0, 1, 100)\n",
    "    r2 = np.linspace(0, 1, 100)\n",
    "    R1, R2 = np.meshgrid(r1, r2)\n",
    "    Z = [WSM(R1, R2), WPM(R1, R2), MinMax(R1, R2)]\n",
    "\n",
    "    # plot the reward function as a 2D heatmap\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    methods = ['WSM', 'WPM', 'MinMax']\n",
    "\n",
    "    for i in range(3):\n",
    "        cs = axs[i].contourf(R1, R2, Z[i], 50, cmap='cividis')\n",
    "        contour = axs[i].contour(R1, R2, Z[i], levels=[0.01, 0.1, 0.25, 0.5, 0.75, 1, 2, 4, 16], colors='white', linewidths=0.5)\n",
    "        axs[i].set_xlabel('r1')\n",
    "        axs[i].set_ylabel('r2')\n",
    "        cbar = fig.colorbar(cs, ax=axs[i])\n",
    "        # cbar.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "        # axs[i].clabel(contour, inline=1, fontsize=8)\n",
    "        axs[i].set_aspect('equal')\n",
    "        axs[i].set_title(methods[i])\n",
    "\n",
    "    plt.tight_layout()  # Adjust the layout\n",
    "    plt.show()\n",
    "\n",
    "plot_shared_reward_function()\n",
    "\n",
    "\n",
    "# Define the functions\n",
    "def WSM(r1, r2):\n",
    "    return r1 + r2\n",
    "\n",
    "def WPM(r1, r2):\n",
    "    return (r1 * r2)\n",
    "\n",
    "def MinMax(r1, r2):\n",
    "    return np.minimum(r1, r2)\n",
    "\n",
    "# Define the plotting function\n",
    "def plot_gradient_field():\n",
    "    r1 = np.linspace(0.01, 1, 15)  # Avoid zero to prevent undefined gradients for WPM\n",
    "    r2 = np.linspace(0.01, 1, 15)\n",
    "    R1, R2 = np.meshgrid(r1, r2)\n",
    "\n",
    "    # Calculate the function values\n",
    "    Z_WSM = WSM(R1, R2)\n",
    "    Z_WPM = WPM(R1, R2)\n",
    "    Z_MinMax = MinMax(R1, R2)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # Calculate and plot the gradient fields\n",
    "    for i, Z in enumerate([Z_WSM, Z_WPM, Z_MinMax]):\n",
    "        grad_r1, grad_r2 = np.gradient(Z)\n",
    "\n",
    "        axs[i].quiver(R2, R1, grad_r1, grad_r2)\n",
    "        axs[i].plot(r1, r1, 'r--')  # Equity line where r1 = r2\n",
    "        axs[i].set_title(['WSM Gradient', 'WPM Gradient', 'MinMax Gradient'][i])\n",
    "        axs[i].set_xlabel('r1')\n",
    "        axs[i].set_ylabel('r2')\n",
    "        axs[i].set_aspect('equal')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the plotting function\n",
    "plot_gradient_field()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def WPM(r1, r2):\n",
    "    return r1 * r2\n",
    "\n",
    "def MinMax(r1, r2):\n",
    "    return np.minimum(r1, r2)\n",
    "\n",
    "def plot_equity_contours():\n",
    "    r1 = np.linspace(0.01, 1, 100)\n",
    "    r2 = np.linspace(0.01, 1, 100)\n",
    "    R1, R2 = np.meshgrid(r1, r2)\n",
    "\n",
    "    Z_WPM = WPM(R1, R2)\n",
    "    Z_MinMax = MinMax(R1, R2)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Contour plot for WPM\n",
    "    cs = axs[0].contour(R1, R2, Z_WPM, levels=np.linspace(np.min(Z_WPM), np.max(Z_WPM), 10), cmap='viridis')\n",
    "    axs[0].plot(r1, r1, 'r--', label='r1 = r2')  # Equity line where r1 = r2\n",
    "    axs[0].set_title('WPM Contours')\n",
    "    axs[0].set_xlabel('r1')\n",
    "    axs[0].set_ylabel('r2')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Contour plot for MinMax\n",
    "    cs = axs[1].contour(R1, R2, Z_MinMax, levels=np.linspace(np.min(Z_MinMax), np.max(Z_MinMax), 10), cmap='viridis')\n",
    "    axs[1].plot(r1, r1, 'r--', label='r1 = r2')  # Equity line where r1 = r2\n",
    "    axs[1].set_title('MinMax Contours')\n",
    "    axs[1].set_xlabel('r1')\n",
    "    axs[1].set_ylabel('r2')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_equity_contours()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
